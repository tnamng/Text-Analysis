{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-QYQCqoQQtJ"
   },
   "source": [
    "# Setting and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4398,
     "status": "ok",
     "timestamp": 1615465753809,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "TeZfPl73QQtN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "######################################\n",
    "import random\n",
    "\n",
    "######################################\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "#from pathlib import Path\n",
    "\n",
    "\n",
    "######################################\n",
    "import re, nltk, spacy\n",
    "\n",
    "\n",
    "######################################\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "######################################\n",
    "#from PIL import Image, ImageDraw\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "######################################\n",
    "#import sklearn\n",
    "\n",
    "\n",
    "######################################\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6001,
     "status": "ok",
     "timestamp": 1615465764109,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "Z3KA-5Z4QQtO",
    "outputId": "c421b4d2-c81e-472d-9811-492546283332"
   },
   "outputs": [],
   "source": [
    "# #FOR COLAB\n",
    "# !gdown https://drive.google.com/uc?id=1UG1hY2lxBU62ouiY679972kohWAvOjRM\n",
    "# root_path = os.getcwd()\n",
    "# DATA_PATH = os.path.join(root_path, 'movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJl8xlrkQQtO"
   },
   "outputs": [],
   "source": [
    "#FOR local computer\n",
    "DATA_PATH = 'movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1615465772649,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "t6KOE3MwQQtO"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(DATA_PATH) #, encoding='utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtU0GIzyQQtP"
   },
   "outputs": [],
   "source": [
    "# def stream_docs(path):\n",
    "#     with open(path, 'r', encoding='utf-8') as csv:\n",
    "#         next(csv)  # skip header\n",
    "#         for line in csv:\n",
    "#             text, label = line[:-3], int(line[-2])\n",
    "#             yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB6oUQLaQQtP"
   },
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1615465776524,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "W-Fn8Qo2QQtQ",
    "outputId": "8b0a1738-1f31-405f-863f-6b0a8f6ac488"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1615465781074,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "PcYx-FDPQQtR",
    "outputId": "e6109c73-f229-4219-f4fe-a8899c2bd155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1615465783710,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "ux8dPXb6QQtR",
    "outputId": "958dccc0-1b97-47e7-f380-1353ec4a67f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1615465786734,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "4j04cj6_QQtS",
    "outputId": "d857e45b-8bb4-426d-89e5-a8f696c73088"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1615465789608,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "gg1Al2ceQQtS",
    "outputId": "8c193a48-5e83-49a9-ade3-ad8624cd0bb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81VitarLQQtU"
   },
   "source": [
    "# Clearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1615465918167,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "HnQ94gCSQQtU"
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)  \n",
    "    #Reomove all HTML markup, Advanced see html.parser in python\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) #List cac emoticons\n",
    "    #text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "    #        ' '.join(emoticons).replace('-', ''))\n",
    "    text =  re.sub('[\\W]+', ' ', text.lower()) \n",
    "    #loai bo cac ky tu khong la word, trong do co cac emotions\n",
    "    text = text + ' '.join(emoticons).replace('-', '')#keep emotion o dang sau\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1615465920520,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "qZMLGPmQQQtV",
    "outputId": "5a212ce1-6a75-42c4-bc6e-73abf63dec31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(raw_df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "executionInfo": {
     "elapsed": 1096,
     "status": "ok",
     "timestamp": 1615465923375,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "dbcrgbniQQtW",
    "outputId": "52f6233e-2f1c-4d53-fe8a-c03e76cfa971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5197,
     "status": "ok",
     "timestamp": 1615465930598,
     "user": {
      "displayName": "Thanh Nam NGUYEN",
      "photoUrl": "",
      "userId": "03809949562924165279"
     },
     "user_tz": -60
    },
    "id": "xozGmc4CQQtW"
   },
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2d1bac7ef0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Step 1: Create a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ds_raw = tf.data.Dataset.from_tensor_slices(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4510\u001b[0m         \u001b[0;36m3\u001b[0m  \u001b[0mmonkey\u001b[0m        \u001b[0mNaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m         \"\"\"\n\u001b[0;32m-> 4512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4514\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a dataset\n",
    "# Step 1: Create a dataset\n",
    "\n",
    "target = df.pop('sentiment')\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    (df.values, target.values))\n",
    "\n",
    "## inspection:\n",
    "for ex in ds_raw.take(3):\n",
    "    tf.print(ex[0].numpy()[0][:50], ex[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "ds_raw = ds_raw.shuffle(\n",
    "    50000, reshuffle_each_iteration=False)\n",
    "\n",
    "ds_raw_test = ds_raw.take(25000)\n",
    "ds_raw_train_valid = ds_raw.skip(25000)\n",
    "ds_raw_train = ds_raw_train_valid.take(20000)\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(\n",
    "    ds_raw_train, \n",
    "    ds_raw_valid, \n",
    "    ds_raw_test,\n",
    "    max_seq_length=None,\n",
    "    batch_size=32):\n",
    "    \n",
    "    ## Step 1: (already done => creating a dataset)\n",
    "    ## Step 2: find unique tokens\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "    token_counts = Counter()\n",
    "\n",
    "    for example in ds_raw_train:\n",
    "        tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "        if max_seq_length is not None:\n",
    "            tokens = tokens[-max_seq_length:]\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "    print('Vocab-size:', len(token_counts))\n",
    "\n",
    "\n",
    "    ## Step 3: encoding the texts\n",
    "    encoder = tfds.features.text.TokenTextEncoder(token_counts)\n",
    "    def encode(text_tensor, label):\n",
    "        text = text_tensor.numpy()[0]\n",
    "        encoded_text = encoder.encode(text)\n",
    "        if max_seq_length is not None:\n",
    "            encoded_text = encoded_text[-max_seq_length:]\n",
    "        return encoded_text, label\n",
    "\n",
    "    def encode_map_fn(text, label):\n",
    "        return tf.py_function(encode, inp=[text, label], \n",
    "                              Tout=(tf.int64, tf.int64))\n",
    "\n",
    "    ds_train = ds_raw_train.map(encode_map_fn)\n",
    "    ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "    ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "    ## Step 4: batching the datasets\n",
    "    train_data = ds_train.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    valid_data = ds_valid.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    test_data = ds_test.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    return (train_data, valid_data, \n",
    "            test_data, len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 48365\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "embedding_dim = 20\n",
    "max_seq_length = 100\n",
    "\n",
    "train_data, valid_data, test_data, n = preprocess_datasets(\n",
    "    ds_raw_train, ds_raw_valid, ds_raw_test, \n",
    "    max_seq_length=max_seq_length, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b-tZGWAQQtZ"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rj3JQ2DQQte"
   },
   "source": [
    "### Embedding layers for sentence encoding\n",
    "\n",
    "\n",
    " * `input_dim`: number of words, i.e. maximum integer index + 1.\n",
    " * `output_dim`: \n",
    " * `input_length`: the length of (padded) sequence\n",
    "    * for example, `'This is an example' -> [0, 0, 0, 0, 0, 0, 3, 1, 8, 9]`   \n",
    "    => input_lenght is 10\n",
    " \n",
    " \n",
    "\n",
    " * When calling the layer, takes integr values as input,   \n",
    " the embedding layer convert each interger into float vector of size `[output_dim]`\n",
    "   * If input shape is `[BATCH_SIZE]`, output shape will be `[BATCH_SIZE, output_dim]`\n",
    "   * If input shape is `[BATCH_SIZE, 10]`, output shape will be `[BATCH_SIZE, 10, output_dim]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1XAB2RQQte"
   },
   "outputs": [],
   "source": [
    "Image(filename='images/16_10.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_xzx8V1QQte"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=100,\n",
    "                    output_dim=6,\n",
    "                    input_length=20,\n",
    "                    name='embed-layer'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyOpb60RQQtf"
   },
   "source": [
    "### Building an RNN model\n",
    "\n",
    "* **Keras RNN layers:**\n",
    "  * `tf.keras.layers.SimpleRNN(units, return_sequences=False)`\n",
    "  * `tf.keras.layers.LSTM(..)`\n",
    "  * `tf.keras.layers.GRU(..)`\n",
    "  * `tf.keras.layers.Bidirectional()`\n",
    " \n",
    "* **Determine `return_sequenes=?`**\n",
    "  * In a multi-layer RNN, all RNN layers except the last one should have `return_sequenes=True`\n",
    "  * For the last RNN layer, decide based on the type of problem: \n",
    "     * many-to-many: -> `return_sequences=True`\n",
    "     * many-to-one : -> `return_sequenes=False`\n",
    "     * ..\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2v1UjC_CQQtf"
   },
   "outputs": [],
   "source": [
    "## An example of building a RNN model\n",
    "## with SimpleRNN layer\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XQZNA7LQQtf"
   },
   "outputs": [],
   "source": [
    "## An example of building a RNN model\n",
    "## with LSTM layer\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijEIZ1nMQQtg"
   },
   "outputs": [],
   "source": [
    "## An example of building a RNN model\n",
    "## with GRU layer\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPWLywfpQQtg"
   },
   "source": [
    "### Building an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqtNVC1HQQtg"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(token_counts) + 2\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "## build the model\n",
    "bi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        name='embed-layer'),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
    "        name='bidir-lstm'), \n",
    "\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "## compile and train:\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = bi_lstm_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs=10)\n",
    "\n",
    "## evaluate on the test data\n",
    "test_results= bi_lstm_model.evaluate(test_data)\n",
    "print('Test Acc.: {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Av4ikrQQQtg"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "\n",
    "bi_lstm_model.save('models/Bidir-LSTM-full-length-seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OL6_4vpQQth"
   },
   "source": [
    " * **Trying SimpleRNN with short sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj1qlb3hQQth"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8gYy7sTQQti"
   },
   "outputs": [],
   "source": [
    "def build_rnn_model(embedding_dim, vocab_size,\n",
    "                    recurrent_type='SimpleRNN',\n",
    "                    n_recurrent_units=64,\n",
    "                    n_recurrent_layers=1,\n",
    "                    bidirectional=True):\n",
    "\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # build the model\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            name='embed-layer')\n",
    "    )\n",
    "    \n",
    "    for i in range(n_recurrent_layers):\n",
    "        return_sequences = (i < n_recurrent_layers-1)\n",
    "            \n",
    "        if recurrent_type == 'SimpleRNN':\n",
    "            recurrent_layer = SimpleRNN(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='simprnn-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            recurrent_layer = LSTM(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='lstm-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'GRU':\n",
    "            recurrent_layer = GRU(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='gru-layer-{}'.format(i))\n",
    "        \n",
    "        if bidirectional:\n",
    "            recurrent_layer = Bidirectional(\n",
    "                recurrent_layer, name='bidir-'+recurrent_layer.name)\n",
    "            \n",
    "        model.add(recurrent_layer)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUI41bmuQQti"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dim = 20\n",
    "max_seq_length = 100\n",
    "\n",
    "train_data, valid_data, test_data, n = preprocess_datasets(\n",
    "    ds_raw_train, ds_raw_valid, ds_raw_test, \n",
    "    max_seq_length=max_seq_length, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "vocab_size = n + 2\n",
    "\n",
    "rnn_model = build_rnn_model(\n",
    "    embedding_dim, vocab_size,\n",
    "    recurrent_type='SimpleRNN', \n",
    "    n_recurrent_units=64,\n",
    "    n_recurrent_layers=1,\n",
    "    bidirectional=True)\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NqIwLUqQQti"
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = rnn_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKZNR6sUQQtj"
   },
   "outputs": [],
   "source": [
    "results = rnn_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kzli_wy9QQtj"
   },
   "outputs": [],
   "source": [
    "print('Test Acc.: {:.2f}%'.format(results[1]*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "81VitarLQQtU",
    "3Rj3JQ2DQQte",
    "oyOpb60RQQtf",
    "RPWLywfpQQtg",
    "13aumwBDQQtj",
    "U0fyEYIxQQtk",
    "15C7KarbQQtl"
   ],
   "name": "Sentiment analysis with Scikit-learn (non-odred).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf_lab] *",
   "language": "python",
   "name": "conda-env-tf_lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "228.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
